{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d896fc70",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<h1><b>Math-Differential_Calculus:</b></h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19fc59f",
   "metadata": {},
   "source": [
    "## 1. Derivative explain:\n",
    "\n",
    "lets say if a car travels at 10km per hr for first hour and 20 km per hour for second hour. Then the car speed is 15km/hr but it instantenous speed is 10km/hr at 1hr exactly and 20km/hr at 2nd hour. And it doesn't consider the change that has already done.\n",
    "\n",
    "2. Integration explain:\n",
    "Integration = Adding up the small distances at each moment to get total distance\n",
    "\n",
    "So, from:\n",
    "\n",
    "Home ‚Üí School = 2 km\n",
    "\n",
    "School ‚Üí Market = 1.5 km\n",
    "\n",
    "Market ‚Üí Home = 2.5 km\n",
    "\n",
    "The total journey distance =\n",
    "\n",
    "Distance\n",
    "=\n",
    "2\n",
    "+\n",
    "1.5\n",
    "+\n",
    "2.5\n",
    "=\n",
    "6\n",
    "¬†km\n",
    "Distance=2+1.5+2.5=6¬†km\n",
    "This is what integration gives you if you knew how fast the kid was walking at each moment.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. combined explanation:\n",
    "\n",
    "so integration is like knowing how much the stock prices has moved from morning to evening but differentiation is like knowning how much the stock prices deflected at different movements as we can see that in graphs\n",
    "\n",
    "$Important$: Differentiation is the process of finding the derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9058973f",
   "metadata": {},
   "source": [
    "slope = A derivative is a slope as it tells about the rate of change.\n",
    "        for example y=mx\n",
    "        for a input x how often y changes, with repect to the m.\n",
    "        if m=1 i.e, slope is at 45degree ,as the y changes 1 unit for 1 unit change in x\n",
    "        \n",
    "Why does a derivative needs only two points to define a slope or why is only two points are used for a derivative?\n",
    "\n",
    "- A derivative is the best straight-line slope at one point. Two points define a line. Three points may not lie on the same line, so they define a curve, not a slope at a point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e74765f",
   "metadata": {},
   "source": [
    "why do we plot slopes?\n",
    "- we plot slopes to check the values of derivatives at different instances.\n",
    "- like a slope is formed between two instances of a derivative function, it gives a straight line so we can calculate where might the other values lie on that slope.\n",
    "\n",
    "-  they are simply the output values of the derivative function, which itself comes from the original function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7752a6",
   "metadata": {},
   "source": [
    "## Defining the slope of a curve\n",
    "\n",
    "- Here we want to find the best possible slope for a single point on a curve.\n",
    "- in simple terms we want to find the tangent on that line, which is the best slope for that point, however to do this we create another point and as the point approaches infinitsimally small distance near to the first point or equal to that point it shows the best slope i.e., tangent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fc0677",
   "metadata": {},
   "source": [
    "## Differentiablity\n",
    "\n",
    "- The function $f(x)=|x|$ is said to be **non-differentiable** at $x=0$: its derivative is undefined at $x=0$. This means that the curve $y=|x|$ has an undefined slope at that point. However, the function $f(x)=|x|$ is **differentiable** at all other points.\n",
    "- Even though the function y = |x| is continuous \n",
    "- whole paragraph only wants to tell that a function which is continous can be indifferentiable\n",
    "\n",
    "In order for a function to be differentiable at some point , the slope of the line must approach a single finite value as gets infinitely close to \n",
    "\n",
    "This implies several constraints:\n",
    "\n",
    "- The function must be defined at the point, meaning it actually has a value there.\n",
    "\n",
    "- The function must be continuous, meaning its graph is an unbroken line with no jumps or gaps.\n",
    "\n",
    "- The function must be smooth, meaning it can't have a sharp \"breaking point\" where the slope changes abruptly.\n",
    "\n",
    "- The function's slope cannot be vertical, because a vertical line has an infinite, undefined slope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d15a15",
   "metadata": {},
   "source": [
    "## Differentiating a function:\n",
    "\n",
    "How does a function is differentiated?\n",
    "\n",
    "- Is there any standard way to differentiate all functions or all different functions has their unique way to differentiating . so the answer is there is one standard, fundamental way to differentiate all functions, but in practice, we use a toolkit of unique rules for different types of functions.\n",
    "\n",
    "- Rules like derivative rule, product rule , etc, formulas for differentiation are derived from that function.\n",
    "\n",
    "<hr />\n",
    "\n",
    "The **derivative** of a function $f(x)$ at $x = x_\\mathrm{A}$ is noted $f'(x_\\mathrm{A})$, and it is defined as:\n",
    "\n",
    "$f'(x_\\mathrm{A}) = \\underset{x_\\mathrm{B} \\to x_\\mathrm{A}}\\lim\\dfrac{f(x_\\mathrm{B}) - f(x_\\mathrm{A})}{x_\\mathrm{B} - x_\\mathrm{A}}$\n",
    "\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b150dca",
   "metadata": {},
   "source": [
    "## Chain Rule:\n",
    "\n",
    "In chain rule , first the outermost derivative is calculated then the second and so on at last the innermost.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75985a74",
   "metadata": {},
   "source": [
    "## Derivatives and optimization\n",
    "\n",
    "Optimization is simply the process of finding the very best value for a function. This usually means finding its highest point (the maximum) or its lowest point (the minimum).\n",
    "\n",
    "Gradient Descent method:\n",
    "- In this method , we start from a random point on the curve and slowly move down and check where the value is minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed24c1",
   "metadata": {},
   "source": [
    "## Higher order derivatives:\n",
    "\n",
    "it is like finding the derivative of another derivative\n",
    "\n",
    "like- f'(x) is the first order derivative , f''(x) is second and f'''(x) is third.\n",
    "\n",
    "It just represent the instantenous rate of change of the rate of change itself.\n",
    "\n",
    "for example, lets say that- f'(x) represents change in position with respect to time i.e., speed, so its second order derivative f''(x) will represnt change in speed i.e., acceleration.\n",
    "\n",
    "we only use upto 2nd order derivative in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e8e33e",
   "metadata": {},
   "source": [
    "## Partial Derivatives:\n",
    "\n",
    "so the partial derivative is just about calculating the slope, when out point at which we want to find slope have three or more directions.\n",
    "\n",
    ":-Before understandng partial derivatives we have to understand derivatives,curves,slopes again \n",
    "\n",
    "__what is the purpose of finding derivatives,what is a slope exactly, why we find slope at a single point not two for a curve, why do we need only one line if a single point can have infinite, what is a tangent line, why not take tangent for a straight line?__\n",
    "\n",
    "- The purpose of finding derivatives for a function is to find the instantenous rate of change at a specific instant, because this is helpful for finding the best values of a function\"maximum and minimum\" , and also to predict how the function will behave and what direction it points on.\n",
    "\n",
    "- The slope of a line is just a number which describes the steepness of that line.\n",
    "\n",
    "- we find a single slope at a single instant because calculating a slope between two points gives the average slope over the distance but when we need to find behaviour of function we need to know the exact perfect slope at a single moment not an average, and that single moment is our single point.\n",
    "\n",
    "- For a single point on a curve there are infinte possible lines that can pass through it, but we didn't just want any line we need one specific line that perfectly represents the curve's direction at that instant, and that line is tangent line.\n",
    "\n",
    "- A tangent line is a line that touches a curve at a single point, going in the same direction as the curve at that exact spot.\n",
    "\n",
    "- for a straight line at any point whereever we draw a tangent it points in the same direction of the line, so for a straight line it itself is tangent.  \n",
    "in short, thr slope is just the steepness of any line but the derivative is the perfect steepness of the tangent line(this can be called as geometrical or graphical interpretation of derivatives).\n",
    "\n",
    "__summary :__\n",
    "When we calculate using two points, we are basically finding the basic average slope between them.\n",
    "When we calculate for a single point on a curve,then we are finding the derivative, which gives us the instantaneous slope at that precise moment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0209c9",
   "metadata": {},
   "source": [
    "## Derivative at a Single Point\n",
    "\n",
    "To calculate the **derivative at a single point**, we start by picking **two points** on the curve,  \n",
    "and then use a **limit** to bring them **infinitely close**, until they nearly become **one point**.  \n",
    "This process transforms the **average slope** between two points into the **exact slope** at a single point ‚Äî which is the **derivative**.\n",
    "\n",
    "---\n",
    "\n",
    "###  Example:\n",
    "\n",
    "Let‚Äôs find the derivative of:\n",
    "\n",
    "$$\n",
    "f(x) = x^2 \\quad \\text{at} \\quad x = a\n",
    "$$\n",
    "\n",
    "We use the **difference quotient** formula:\n",
    "\n",
    "$$\n",
    "f'(a) = \\lim_{h \\to 0} \\frac{f(a + h) - f(a)}{h}\n",
    "$$\n",
    "\n",
    "Now plug in the function:\n",
    "\n",
    "$$\n",
    "f'(a) = \\lim_{h \\to 0} \\frac{(a + h)^2 - a^2}{h}  \n",
    "= \\lim_{h \\to 0} \\frac{a^2 + 2ah + h^2 - a^2}{h}  \n",
    "= \\lim_{h \\to 0} \\frac{2ah + h^2}{h}  \n",
    "= \\lim_{h \\to 0}(2a + h)  \n",
    "= 2a\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "So the derivative of \\( f(x) = x^2 \\) at any point \\( x = a \\) is:\n",
    "\n",
    "$$\n",
    "f'(a) = 2a\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cb82fe",
   "metadata": {},
   "source": [
    "## Partial Derivatives:\n",
    "\n",
    "Now we can truly understand the concept of partial derivatives.\n",
    "\n",
    "- For a single point on a curve on a 2D plane/axis when we find derivative of a line , that line has only one possible tangent.  \n",
    "But for a point on a curve in 3d plane can have infinite number of tangent lines\n",
    "\n",
    "- In simple way ,it means that in 3d axis the x, y are just axis of a 3d plane like a folding sheet, and when we move those x and y we reach at different z at each point on the sheet.\n",
    "\n",
    "- like we used to set input x values to get the y output , in partial derivatives we set x and y both to get z output. by these input and ouput values an imaginary plane is created like the folding sheet.\n",
    "\n",
    "In partial derivative we find the rate of change(slope) of a function while keeping while keeping all other variable as constant.\n",
    "\n",
    "## üîπ Partial Derivative Example\n",
    "\n",
    "Let's take a multivariable function:\n",
    "\n",
    "    f(x, y) = x¬≤y + 3y\n",
    "\n",
    "This means `f` depends on **two variables**, `x` and `y`.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Partial Derivative with respect to `x`:\n",
    "\n",
    "We treat `y` as a **constant** and only differentiate with respect to `x`:\n",
    "\n",
    "    ‚àÇf/‚àÇx = ‚àÇ/‚àÇx (x¬≤y + 3y)\n",
    "          = 2xy\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Partial Derivative with respect to `y`:\n",
    "\n",
    "Now we treat `x` as a **constant** and differentiate with respect to `y`:\n",
    "\n",
    "    ‚àÇf/‚àÇy = ‚àÇ/‚àÇy (x¬≤y + 3y)\n",
    "          = x¬≤ + 3\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Final Answers:\n",
    "\n",
    "- ‚àÇf/‚àÇx = 2xy  \n",
    "- ‚àÇf/‚àÇy = x¬≤ + 3\n",
    "\n",
    "This tells us how the function changes if we change only `x`, or only `y`, one at a time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d756c0",
   "metadata": {},
   "source": [
    "## Gradient:\n",
    "\n",
    "The Gradient is a function that tells about the steepest instantaneous increase.\n",
    "Simply, it tells about the nearest steepest increase at a point A.\n",
    "\n",
    "For example, lets say we have a function with n variables and for convenience we will define a vector x whose components are these variables.\n",
    "\n",
    "$\\mathbf{x}=\\begin{pmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{pmatrix}$ \n",
    "\n",
    "Now $f(\\mathbf{x})$ is easier to write than $f(x_1, x_2, \\dots, x_n)$.\n",
    "\n",
    "The gradient of the function $f(\\mathbf{x})$ at some point $\\mathbf{x}_\\mathrm{A}$ is the vector whose components are all the partial derivatives of the function at that point. It is noted $\\nabla f(\\mathbf{x}_\\mathrm{A})$, or sometimes $\\nabla_{\\mathbf{x}_\\mathrm{A}}f$:\n",
    "\n",
    "$\\nabla f(\\mathbf{x}_\\mathrm{A}) = \\begin{pmatrix}\n",
    "\\dfrac{\\partial f}{\\partial x_1}(\\mathbf{x}_\\mathrm{A})\\\\\n",
    "\\dfrac{\\partial f}{\\partial x_2}(\\mathbf{x}_\\mathrm{A})\\\\\n",
    "\\vdots\\\\\n",
    "\\dfrac{\\partial f}{\\partial x_n}(\\mathbf{x}_\\mathrm{A})\\\\\n",
    "\\end{pmatrix}$\n",
    "\n",
    "\n",
    "\n",
    "- gradient doesn't tells the x and y axis values , it tell the increase in steepness over x position and y position. like lets say for a f(1, 2) if gradient of f(1, 2) is (3, 4).\n",
    "\n",
    "- In simple terms, it‚Äôs not a point. It‚Äôs a vector with direction and with steepness(magnitude) from your current location. like if for function z = f(x, y) the gradient vector is gradientf(1, 2) = (3, 4) .then the gradient vectors doesn't tell that the steepness is at (3,4) it tell that the steepness is 3unit from x and 4unit from y away from the location of input x and 4 i.e., at (4,6)\n",
    "\n",
    "$ IMPORTANT $ - Gradient descent tells about the steepest increase(uphill), The gradient vector at a point tells you the direction of the steepest and most immediate increase in value (uphill) from that point. The value it gives is just fot the nearest and highest uphill.\n",
    "But that only works if there is a gradual(continuous or without break) increase nearby. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b887a861",
   "metadata": {},
   "source": [
    "## Gradient Descent :\n",
    "\n",
    "Gradient descent tells about the steepest downhill.\n",
    "\n",
    "So the gradient descent is finding the downhill in opposite direction but it is limited to check only the area up to the negative of gradient, it can't go any further.\n",
    "\n",
    "suppose we find gradient of f(1, 2) is (3,4) so we start moving in the oppsite direction(180degree position) ,i.e, the direction in which -3,-4lies and we start taking small steps in that direction like first (0.7,1.6) then (0.4, 1.2) and so on. and where the gradient becomes zero, that point is the steepest downhill or flat.\n",
    "\n",
    "\n",
    "- while training neural networks we keep the training examples in matrix X and labels in vecotr y, both of these are treated as constants. while w is treated as variable. \n",
    "- we try to minimize the loss function(score for wrong prediction) and cost function(average of all wrong prediction scores). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9e5990",
   "metadata": {},
   "source": [
    "### Formulas for Gradient Descent in Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. The Gradient Descent Update Rule**\n",
    "$$\n",
    "\\mathbf{x}_t = \\mathbf{x}_{t-1} - \\eta \\nabla f(\\mathbf{x}_{t-1})\n",
    "$$\n",
    "* [cite_start]**Definition:** This is the core formula for the Gradient Descent algorithm[cite: 85].\n",
    "* [cite_start]**What it's for:** It describes how to take one step \"downhill\" on a function to find its minimum value[cite: 85]. [cite_start]This process is repeated many times[cite: 85].\n",
    "* **What it means:**\n",
    "    * [cite_start]`x_t`: Your **new position** after taking the step[cite: 85].\n",
    "    * [cite_start]`x_{t-1}`: Your **current position** before taking the step[cite: 85].\n",
    "    * [cite_start]`Œ∑`: The **learning rate**, a small number that controls the size of your step[cite: 85].\n",
    "    * [cite_start]`‚àáf(x_{t-1})`: The **gradient** of the function at your current position, which points in the steepest \"uphill\" direction[cite: 85]. By subtracting it, you move downhill.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. The Prediction Function**\n",
    "$$\n",
    "\\hat{y} = f_{\\mathbf{w}}(\\mathbf{x})\n",
    "$$\n",
    "* [cite_start]**Definition:** This represents a neural network making a prediction[cite: 85].\n",
    "* [cite_start]**What it's for:** Used to show that the model's prediction depends not only on the input data but also on its internal parameters[cite: 85].\n",
    "* **What it means:**\n",
    "    * [cite_start]`≈∑`: The **prediction** made by the model[cite: 85].\n",
    "    * [cite_start]`x`: The **input data** you give to the model[cite: 85].\n",
    "    * [cite_start]`w`: The **weights** or parameters of the model, which are treated as constants when making a prediction[cite: 85].\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. The Cost (or Loss) Function**\n",
    "$$\n",
    "\\mathcal{L}_{\\mathbf{X},\\mathbf{y}}(\\mathbf{w}) = g(f_{\\mathbf{w}}(\\mathbf{X}), \\mathbf{y})\n",
    "$$\n",
    "* [cite_start]**Definition:** This is the function we want to minimize during training[cite: 85].\n",
    "* [cite_start]**What it's for:** It calculates a single number that measures how \"wrong\" the model's predictions are compared to the true answers[cite: 85].\n",
    "* **What it means:**\n",
    "    * [cite_start]`L(w)`: The **cost or loss**, which depends on the current weights **w**[cite: 85].\n",
    "    * [cite_start]`g(...)`: A function that measures the \"discrepancy\" or error[cite: 85].\n",
    "    * [cite_start]`f_w(X)`: The vector of all predictions the model makes on the training data **X**[cite: 85].\n",
    "    * [cite_start]`y`: The vector of all correct labels or answers[cite: 85]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbeca39",
   "metadata": {},
   "source": [
    "## Jacobians:\n",
    "\n",
    "A jacobian is a function that we use when we work with a vector of scalar functions.\n",
    "It is like a function that takes in multiple input and gives multiple output.\n",
    "Jacobian is like a matrix of gradients of all these functions\n",
    "\n",
    "- For each input, it shows how all outputs respond (i.e., increase or decrease) or it tell with each input how all other outputs changes.\n",
    "\n",
    "### The Jacobian Matrix Formula\n",
    "\n",
    "The Jacobian matrix **J** of a function **f** that takes an n-dimensional vector **x** and produces an m-dimensional vector **f(x)** is the matrix of all first-order partial derivatives.\n",
    "\n",
    "$$\n",
    "\\mathbf{J}_{\\mathbf{f}}(\\mathbf{x}) =\n",
    "\\begin{pmatrix}\n",
    " \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    " \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n} \\\\\n",
    " \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " \\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "* **Rows:** Each row corresponds to one of the output functions ($f_1, f_2, ..., f_m$).\n",
    "* **Columns:** Each column corresponds to one of the input variables ($x_1, x_2, ..., x_n$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f34dc10",
   "metadata": {},
   "source": [
    "## Hessians:\n",
    "\n",
    "A hessian is also called as second order partial derivative .\n",
    "\n",
    "A hessian matrix is matrix that contains all the possible hessians with respect to all varibales in the function.\n",
    "\n",
    "Deep learning almost never uses it because ,If a function has n variables, then there are n^2 hessians. Since neural netwroks typically have several millions of paramters the number of hessians would exceed thousands of billions.\n",
    "\n",
    "A **Hessian** is a square matrix that contains all the second-order partial derivatives of a function. It is the higher-dimensional equivalent of the second derivative.\n",
    "\n",
    "***\n",
    "$$\n",
    "\\mathbf{H}_{\\mathbf{f}}(\\mathbf{x}) =\n",
    "\\begin{pmatrix}\n",
    " \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n",
    " \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n",
    " \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "this is the hessian matrix of a function at some point Xa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cab8fd",
   "metadata": {},
   "source": [
    "## Few proofs:\n",
    "\n",
    "### Common Derivative Formulas\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Power Rule**\n",
    "$$\n",
    "\\frac{d}{dx}[x^r] = rx^{r-1}\n",
    "$$\n",
    "* **Use:** For differentiating a variable raised to a constant power.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Logarithm Rule**\n",
    "$$\n",
    "\\frac{d}{dx}[\\ln(x)] = \\frac{1}{x}\n",
    "$$\n",
    "* **Use:** For differentiating the natural logarithm of a variable.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Product Rule**\n",
    "$$\n",
    "\\frac{d}{dx}[g(x)h(x)] = g(x)h'(x) + g'(x)h(x)\n",
    "$$\n",
    "* **Use:** For differentiating the product of two functions.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Chain Rule**\n",
    "$$\n",
    "\\frac{d}{dx}[g(h(x))] = g'(h(x)) \\cdot h'(x)\n",
    "$$\n",
    "* **Use:** For differentiating a function nested inside another function (a composite function).\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Multiplicative Inverse Rule**\n",
    "$$\n",
    "\\frac{d}{dx}\\left[\\frac{1}{h(x)}\\right] = -\\frac{h'(x)}{[h(x)]^2}\n",
    "$$\n",
    "* **Use:** A specific case for differentiating the reciprocal of a function.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Quotient Rule**\n",
    "$$\n",
    "\\frac{d}{dx}\\left[\\frac{g(x)}{h(x)}\\right] = \\frac{g'(x)h(x) - g(x)h'(x)}{[h(x)]^2}\n",
    "$$\n",
    "* **Use:** For differentiating the division of two functions.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Constant Rule**\n",
    "$$\n",
    "\\frac{d}{dx}[c] = 0\n",
    "$$\n",
    "* **Use:** The derivative of any constant number is zero.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8ed81",
   "metadata": {},
   "source": [
    "### Comprehensive Derivative Formulas\n",
    "\n",
    "---\n",
    "\n",
    "#### **Basic Operational Rules**\n",
    "\n",
    "* **Sum/Difference Rule:**\n",
    "    $$\n",
    "    \\frac{d}{dx}[f(x) \\pm g(x)] = f'(x) \\pm g'(x)\n",
    "    $$\n",
    "\n",
    "* **Constant Multiple Rule:**\n",
    "    $$\n",
    "    \\frac{d}{dx}[k \\cdot f(x)] = k \\cdot f'(x)\n",
    "    $$\n",
    "\n",
    "* **Product Rule:**\n",
    "    $$\n",
    "    \\frac{d}{dx}[f(x)g(x)] = f(x)g'(x) + g(x)f'(x)\n",
    "    $$\n",
    "\n",
    "* **Quotient Rule:**\n",
    "    $$\n",
    "    \\frac{d}{dx}\\left[\\frac{f(x)}{g(x)}\\right] = \\frac{g(x)f'(x) - f(x)g'(x)}{[g(x)]^2}\n",
    "    $$\n",
    "\n",
    "* **Chain Rule:**\n",
    "    $$\n",
    "    \\frac{d}{dx}[f(g(x))] = f'(g(x)) \\cdot g'(x)\n",
    "    $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Derivatives of Common Functions**\n",
    "\n",
    "* **Constant Rule:**\n",
    "    $$\n",
    "    \\frac{d}{dx}[k] = 0\n",
    "    $$\n",
    "\n",
    "* **Power Rule:**\n",
    "    $$\n",
    "    \\frac{d}{dx}[x^n] = nx^{n-1}\n",
    "    $$\n",
    "\n",
    "* **Exponential Functions:**\n",
    "    $$\n",
    "    \\frac{d}{dx}[e^x] = e^x\n",
    "    $$\n",
    "    $$\n",
    "    \\frac{d}{dx}[a^x] = a^x \\ln a\n",
    "    $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Derivatives of Trigonometric Functions**\n",
    "\n",
    "* **Sine:**\n",
    "    $$\n",
    "    \\frac{d}{dx}[\\sin x] = \\cos x\n",
    "    $$\n",
    "\n",
    "* **Cosine:**\n",
    "    $$\n",
    "    \\frac{d}{dx}[\\cos x] = -\\sin x\n",
    "    $$\n",
    "\n",
    "* **Tangent:**\n",
    "    $$\n",
    "    \\frac{d}{dx}[\\tan x] = \\sec^2 x\n",
    "    $$\n",
    "\n",
    "* **Cotangent:**\n",
    "    $$\n",
    "    \\frac{d}{dx}[\\cot x] = -\\csc^2 x\n",
    "    $$\n",
    "\n",
    "* **Secant:**\n",
    "    $$\n",
    "    \\frac{d}{dx}[\\sec x] = \\sec x \\tan x\n",
    "    $$\n",
    "\n",
    "* **Cosecant:**\n",
    "    $$\n",
    "    \\frac{d}{dx}[\\csc x] = -\\csc x \\cot x\n",
    "    $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Derivatives of Inverse Trigonometric Functions**\n",
    "\n",
    "* **Arcsine (Inverse Sine):**\n",
    "    $$\n",
    "    \\frac{d}{dx}[\\sin^{-1} x] = \\frac{1}{\\sqrt{1-x^2}}\n",
    "    $$\n",
    "\n",
    "* **Arccosine (Inverse Cosine):**\n",
    "    $$\n",
    "    \\frac{d}{dx}[\\cos^{-1} x] = -\\frac{1}{\\sqrt{1-x^2}}\n",
    "    $$\n",
    "\n",
    "* **Arctangent (Inverse Tangent):**\n",
    "    $$\n",
    "    \\frac{d}{dx}[\\tan^{-1} x] = \\frac{1}{1+x^2}\n",
    "    $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
